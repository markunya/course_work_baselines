# Фреймворк для задачи улучшения речи

Неоффициальная имплементация https://arxiv.org/abs/2410.05920. В данном репозитории представлен код для обучения и экспериментов с рядом современных генеративно-состязательных моделей (GAN) в задачах улучшения качества речи. Фреймворк ориентирован на гибкость, модульность и воспроизводимость экспериментов. Поддерживается оценка качества с использованием актуальных метрик.

## Настройка окружения
```bash
git clone https://github.com/markunya/course_work_baselines.git
cd course_work_baselines
conda create -n myenv python=3.12.8 ffmpeg
source activate myenv
pip install -r requirements.txt
```

## Данные и датасеты

Все датасеты регистрируются через `datasets_registry`, определённый в `datasets/datasets.py`.

#### `mel_dataset`
Датасет используется для задачи инверсии мел-спектрограммы. Основан на оригинальной реализации HiFi-GAN с минимальными изменениями.

**Выход:**
- `mel` — мел-спектрограмма, предназначенная для инверсии
- `wav` — аудиосигнал Ground Truth
- `name` — имя файла
- `mel_for_loss` — мел-спектрограмма для расчёта функции потерь или метрик

> Используется в обучении HiFi-GAN на базе данных **VCTK**

---

#### `vctk`
Применяется в задаче расширения полосы пропускания при обучении **HiFi++**. Основан на оригинальной реализации HiFi++.

**Выход:**
- `input_wav` — обрезанное по частотам аудио (вход модели)
- `wav` — Ground Truth
- `name` — имя файла

---

#### `voicebank`
Используется для задачи подавления шума в обучении **HiFi++**. Код основан на оригинальной реализации HiFi++ и адаптирован под текущие нужды.

**Выход:**
- `input_wav` — зашумлённое аудио (вход модели)
- `wav` — Ground Truth
- `name` — имя файла

> Скрипт загрузки и метаданные доступны в `datasets/metadata/voicebank`

---

#### `augmented_libritts-r`
Применяется в задаче универсального улучшения речи при обучении **стадий 1 и 2** модели **FINALLY**.

**Выход:**
- `input_wav` — искажённое (аугментированное) аудио (вход модели)
- `wav` — Ground Truth
- `name` — имя файла

> Скрипт загрузки и метаданные: `datasets/metadata/LibriTTS-R`

---

#### `augmented_daps`
Используется на **3-й стадии обучения** модели **FINALLY**.

**Выход:**
- `input_wav` — искажённое (аугментированное) аудио (вход модели)
- `wav` — Ground Truth
- `name` — имя файла

> Скрипт загрузки и метаданные: `datasets/metadata/DAPS`

---

#### `finally_dataset`
Датасет для **инференса** модели **FINALLY**. Может быть использован с любым набором `.wav` файлов.

**Выход:**
- `input_wav` — входной аудиосигнал
- `name` — имя файла

---

#### `vctk-demand`
Используется для **валидации** модели FINALLY в задаче универсального улучшения речи.

**Выход:**
- `input_wav` — загрязнённое аудио (вход модели)
- `wav` — Ground Truth
- `name` — имя файла

> Скрипт загрузки и метаданные: `datasets/metadata/VCTK-Demand`

## Аугментации

Все аугментации регистрируются в `augmentations_registry`, определённом в `datasets/augmentations.py`.

#### `noise`
Добавляет к аудиозаписи случайный шум. Используемые шумы взяты из датасета **DNS Challenge**.

- Источник шумов: `datasets/metadata/DNS_noise`
- Скрипт скачивания и метаданные доступны в соответствующей папке

---

### `impulse_response`
Имитация акустики помещения и характеристик микрофона путём свёртки сигнала с импульсной характеристикой.

- Для моделирования помещений используются импульсные характеристики из DNS Challenge: `datasets/metadata/DNS_ir`
- Для моделирования микрофонов: `datasets/metadata/room_ir`
- Скрипты скачивания и описания доступны в соответствующих директориях

---

#### `acrusher`
Эффект биткрашинга — цифровое снижение битовой глубины сигнала. Уменьшает число доступных амплитудных уровней, создавая характерные жёсткие артефакты квантования и делая звук "цифровым" и "ломаным".

---

#### `crystalizer`
Выделяет и усиливает высокочастотные компоненты сигнала. Повышает субъективную чёткость звучания, но может вызывать артефакты и искажения, особенно при чрезмерном применении.

---

#### `flanger`
Наложение сигнала на его слегка задержанную и модулированную копию. Создаёт эффект "волнообразного" звучания или "реактивного пролёта". Часто используется в музыкальной обработке для создания пространства.

---

#### `vibrato`
Периодическая модуляция частоты сигнала, вызывающая дрожание высоты тона. Эффект близок к естественному вибрато в человеческой речи или пении.

---

#### `codec` (ogg, mp3)
Применение кодеков с потерями (Vorbis или MP3) для имитации деградации качества, характерной при сжатии звука. Используются различные битрейты для разнообразия артефактов.

---

Прослушать эффект применения каждой аугментации можно в ноутбуке datasets/augmentations.ipynb.

## Модели

Все модели, напрямую используемые в обучении и инференсе, регистрируются в `models_registry`, определённом в `models/models.py`.

#### `hifigan_gen`
Генератор из оригинальной статьи HiFi-GAN. Используется для задачи инверсии мел-спектрограммы в аудиосигнал.

---

#### `mpd`
Multi-Period Discriminator — многопериодный дискриминатор из HiFi-GAN. Оценивает качество синтезированного сигнала на различных временных интервалах. Используется совместно с `hifigan_gen`.

---

#### `msd`
Multi-Scale Discriminator — многомасштабный дискриминатор из HiFi-GAN. Работает на различных уровнях разрешения сигнала. Применяется как в HiFi-GAN, так и в HiFi++.

---

#### `hifi++_gen`
Генератор HiFi++ для задачи инверсии мел-спектрограммы. Расширяет архитектуру HiFi-GAN с улучшениями по стабильности и качеству.

---

#### `a2a_hifi++_gen`
Модифицированный генератор HiFi++ для задач **речевого улучшения** — в частности, расширения полосы пропускания и удаления шума. Принимает на вход аудиосигнал, а не спектрограмму.

---

#### `finally_gen`
Генератор FINALLY для универсального улучшения речи. Объединяет в себе архитектуру HiFi++ и дополнительную обработку на основе признаков WavLM. Поддерживает дообучение на различных стадиях.

---

#### `ms-stft_disc`
Multi-Scale STFT Discriminator — многомасштабный дискриминатор, работающий в спектральной области (на STFT-преобразовании сигнала). Используется при обучении модели `finally_gen` для оценки качества синтезированного звука на основе частотного представления.

## Функции потерь

Все функции потерь регистрируются в `losses_registry`, определённом в `training/losses/losses.py`.

#### `feature_loss`
Feature Matching Loss — генераторная функция потерь. Вычисляется как L1-расстояние между скрытыми представлениями всех слоёв дискриминатора для реального и сгенерированного аудио. Улучшает стабильность состязательного обучения, побуждая генератор приближаться к истинному распределению фичей.

---

#### `gen_loss`
LS-GAN генераторная функция потерь. Стремится заставить дискриминатор воспринимать сгенерированные данные как реальные.

---

#### `disc_loss`
LS-GAN дискриминаторная функция потерь. Обучает дискриминатор отличать реальные аудиосигналы от синтетических.

---

#### `l1_mel_loss`
Простая L1-потеря между мел-спектрограммами сгенерированного и реального аудио. Используется в задачах, где входной и целевой сигнал выражаются в спектральной области.

---

#### `lmos`
Потеря на основе **WavLM-Conv** признаков, предложенная в оригинальной статье FINALLY. Состоит из двух компонентов:
- L2-потеря между скрытыми признаками WavLM для реального и сгенерированного аудио;
- L1-потеря между спектрограммами (STFT-модулями) аудио.

---

#### `pesq`
Аппроксимированная дифференцируемая версия метрики PESQ (Perceptual Evaluation of Speech Quality). Позволяет использовать PESQ в качестве функции потерь.

---

#### `utmos`
Функция потерь, использующая предсказание **UTMOSv2** как прокси-метрику субъективного качества речи. Подходит для оптимизации на основе перцептивного восприятия.

## Тренера

Все тренеры регистрируются в `trainers_registry`, определённом в `training/trainers/base_trainer.py`.  
Каждый тренер наследуется от абстрактного класса `BaseTrainer`, который агрегирует общие механизмы:
- инициализация моделей, оптимизаторов и функций потерь;
- логирование (через wandb/console);
- сохранение и загрузка чекпоинтов;
- инференс и оценка метрик.

Каждый конкретный тренер обязан реализовать два метода:
- `train_step` — шаг обучения;
- `synthesize_wavs` — синтез аудио для валидации/логирования.

---

#### `hifigan_trainer`
Тренер для обучения и инференса модели **HiFi-GAN**. Основан на логике из оригинальной реализации. Использует дискриминаторы `mpd` и `msd`.

---

#### `hifi++_trainer`
Тренер для обучения модели **HiFi++** в задачах улучшения речи (bandwidth extension / denoising).

---

#### `finally_stage1_trainer`
Тренер для **первой стадии** обучения модели FINALLY. Использует только регрессионные функции потерь (например, `lmos`). Не поддерживает дискриминаторы.

---

#### `finally_stage2_trainer`
Тренер для **второй стадии** обучения FINALLY. Использует дискриминатор `ms-stft` и поддерживает комбинированные функции потерь (`lmos`, `gen_loss`, `feature_loss` и др.).

---

#### `finally_stage3_pretrainer`
Тренер для **предобучения Upsample WaveUNet**, который добавляется перед третьей стадией. Использует только регрессионные потери. Помогает стабилизировать финальное обучение.

---

#### `finally_stage3_trainer`
Тренер для **третьей стадии** обучения FINALLY. Поддерживает:
- `ms-stft` дискриминаторы;
- регрессионные функции потерь (`lmos`, `utmos`, `pesq`);
- градиентную аккумуляцию (`sub_batch_size`).

## Метрики

Все метрики регистрируются в `metrics_registry`, который находится в `metrics/metrics.py`.  
Для некоторых метрик требуются предварительно обученные веса — они автоматически загружаются с помощью скрипта `metrics/download_extract_weights.sh`.

---

#### `l1_mel_diff`
Простая метрика, измеряющая среднее абсолютное (L1) расстояние между мел-спектрограммами выходного и целевого аудио.

---

#### `wb_pesq` *(Wideband PESQ)*
Классическая объективная метрика качества речи. Сравнивает с ground truth сигналом. Подходит для сигналов с частотой дискретизации 16 кГц и выше.

---

#### `nb_pesq` *(Narrowband PESQ)*
Аналогичная метрика, но предназначена для узкополосных сигналов (например, 8 кГц).

---

#### `stoi` *(Short-Time Objective Intelligibility)*
Метрика разборчивости речи. Хорошо коррелирует с субъективной понятностью речи слушателем. Требует ground truth.

---

#### `si_sdr` *(Scale-Invariant Signal-to-Distortion Ratio)*
Измеряет уровень искажения восстановленного сигнала, независимо от масштаба. Также требует наличие ground truth.

---

#### `mosnet`
Нейросетевая метрика, предсказывающая субъективное восприятие качества (MOS). Требует предварительно обученные веса.

---

#### `utmos`
Модельная MOS-оценка качества речи, основанная на **UTMOSv2**. Не требует ground truth. Предсказывает субъективную оценку.

---

#### `wv-mos`
Предсказание MOS на основе модели **Wav2Vec 2.0**, дообученной на MOS-аннотированных записях. Не требует ground truth.

---

#### `legacy_dnsmos`
Pytorch-реализация старой версии **DNSMOS** без необходимости скачивания весов. Предсказывает качество речи на основе глубокой модели.

---

#### `dnsmos`
Современная версия **DNSMOS**, требующая загрузки весов. Предсказывает качество по нескольким шкалам (e.g., SIG, BAK, OVR).

## Конфигурация

Все эксперименты (обучение, инференс, логирование и т.д.) гибко настраиваются с помощью системы конфигураций **OmegaConf** и YAML-файлов.

---

- Структура конфигов определена в каталоге [`configs/`] — здесь содержится основная схема (`schema.json`), а также примеры и рабочие конфигурации для каждой модели и эксперимента.
- Конфигурации могут быть легко переопределены из командной строки или при помощи Hydra.
- Каждый компонент (модель, датасет, функция потерь, метрика и т.д.) выбирается через поле `name`, совпадающее с названием в соответствующем реестре (например, `models_registry`, `datasets_registry` и т.д.).

## Обучение и инференс
Пример запуска обучения:
```bash
python train.py exp.config_path=config.yaml exp.run_name=train
```

Пример запуска инференса:
```bash
python inference.py exp.config_path=config.yaml exp.run_name=inf
```
